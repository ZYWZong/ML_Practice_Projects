{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 03: Experiments on transformer models"
      ],
      "metadata": {
        "id": "uGcgaawZml6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, I have explored pre-transformer technology (clustering for regression, tree-based models, simple dense neural networks, CNN, and RNN with LSTM and GRU). In this notebook, I experiment on two transformer models on the sentence classification task."
      ],
      "metadata": {
        "id": "HdfytIqhmwyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Notebook 03: Experiments on transformer models](#scrollTo=uGcgaawZml6-)\n",
        "\n",
        ">>[3.1 Load and check dependencies](#scrollTo=YU-tMXP7NrUx)\n",
        "\n",
        ">>>[3.2.1 Load and install dependencies](#scrollTo=HoXBt5hhNzkH)\n",
        "\n",
        ">>>[3.2.2 Load and preprocess data](#scrollTo=e2KR5pTXODRR)\n",
        "\n",
        ">>[3.2 Transformer models](#scrollTo=_uSTqDxLQQ88)\n",
        "\n",
        ">>>[3.2.1 customized BERT model](#scrollTo=9-mekik6wnJk)\n",
        "\n",
        ">>>[3.2.2 customized GPT-2 model](#scrollTo=v1Cjn3yCwvS_)\n",
        "\n",
        ">>[3.3 Conjectures and reasons for failing to achieve an accuracy beyond $40$ percent](#scrollTo=UX6fIN5uwzAz)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "Nrgp1Wzvmx_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Load and check dependencies"
      ],
      "metadata": {
        "id": "YU-tMXP7NrUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Load and install dependencies"
      ],
      "metadata": {
        "id": "HoXBt5hhNzkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for GPU\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqWGCud7mqoF",
        "outputId": "2fbb0038-0e07-4311-e542-ca9d3a960efb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-0ca1e208-4df8-8f79-c99e-ad49084a7f9c)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "Gd7JieG_N-te"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be using the `keras_nlp` library to import pretrained transformer encoders. `keras_nlp` also requires `tensorflow_text` as a dependecy, so we install them here."
      ],
      "metadata": {
        "id": "g7X2A78pnuXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install tensorflow-text"
      ],
      "metadata": {
        "id": "wUTjuCU62Rrz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install keras-nlp"
      ],
      "metadata": {
        "id": "LFxweyvD2iyd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text\n",
        "import keras_nlp"
      ],
      "metadata": {
        "id": "H7u3Dm7y2Wde"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ZYWZong/ML_Practice_Projects/refs/heads/main/SkimLit_project_practice/SkimLit_utils.py\n",
        "\n",
        "from SkimLit_utils import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZGczF5IOA14",
        "outputId": "2a75dd77-78df-4ddc-d3ff-b83e34ccba69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-30 23:03:55--  https://raw.githubusercontent.com/ZYWZong/ML_Practice_Projects/refs/heads/main/SkimLit_project_practice/SkimLit_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7876 (7.7K) [text/plain]\n",
            "Saving to: ‘SkimLit_utils.py’\n",
            "\n",
            "SkimLit_utils.py    100%[===================>]   7.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-30 23:03:56 (72.9 MB/s) - ‘SkimLit_utils.py’ saved [7876/7876]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 Load and preprocess data"
      ],
      "metadata": {
        "id": "e2KR5pTXODRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --quiet https://github.com/Franck-Dernoncourt/pubmed-rct.git"
      ],
      "metadata": {
        "id": "96kkuJ5KOETu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We preprocess the datasets as before."
      ],
      "metadata": {
        "id": "jeB2V8cnVjRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
        "\n",
        "# preprocess the data as in notebook 00\n",
        "train_df, dev_df, test_df = SkimLit_preprocess_master(data_dir)\n",
        "data_all = SkimLit_preprocess_OneHot_NN(train_df, dev_df, test_df)\n",
        "label_Encoded = SkimLit_preprocess_EncodedLabels(train_df,dev_df,test_df)\n",
        "\n",
        "train_sentences = data_all[\"train_text\"]\n",
        "dev_sentences = data_all[\"dev_text\"]\n",
        "test_sentences = data_all[\"test_text\"]\n",
        "\n",
        "train_labels = data_all[\"train_label\"]\n",
        "dev_labels = data_all[\"dev_label\"]\n",
        "test_labels = data_all[\"test_label\"]"
      ],
      "metadata": {
        "id": "XMT9EOhiOIFD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, as before, let's create `tf.tensor` for our datasets."
      ],
      "metadata": {
        "id": "YXZ8vfzRVZ_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels))\n",
        "dev_data = tf.data.Dataset.from_tensor_slices((dev_sentences, dev_labels))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels))"
      ],
      "metadata": {
        "id": "HYaKHtyNVY9N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Transformer models"
      ],
      "metadata": {
        "id": "_uSTqDxLQQ88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I will build two customized transformer models for our sentence classification task, one using BERT and the other using GPT-2."
      ],
      "metadata": {
        "id": "B_Gbwx4GnbRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 customized BERT model"
      ],
      "metadata": {
        "id": "9-mekik6wnJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**!IMPORTANT NOTE!**\n",
        "\n",
        "> PS: After several attempts to build customized layers (including adding various dense, pooling layers) and fine-tuning hyperparameters (learning rate and sequence length), I still couldn't get the model beyond a training accuracy of $40\\%$. So, below, I present my best attempt.\n",
        "\n"
      ],
      "metadata": {
        "id": "7_V4d57BmgBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I experiment on the base BERT encoder model. First, I need to further preprocess the data to be compatible with the BERT encoder. Recall from **notebook 01** that $95\\%$ of our sentences are less than $55$ words, so a sequence length of $55$ should be sufficient."
      ],
      "metadata": {
        "id": "Qs9Y05LNIiDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor_BERT = keras_nlp.models.BertPreprocessor.from_preset(\"bert_base_en_uncased\", sequence_length=55)"
      ],
      "metadata": {
        "id": "WpD7plPzIgPO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for preprocessing data for BERT encoder\n",
        "def preprocess_fn_BERT(sentences, label):\n",
        "    return preprocessor_BERT(sentences), label\n",
        "\n",
        "# again use a batch of 32 and preprocess the data for BERT\n",
        "train_data_BERT = train_data.map(preprocess_fn_BERT).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "dev_data_BERT = dev_data.map(preprocess_fn_BERT).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_data_BERT = test_data.map(preprocess_fn_BERT).batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Qmf2JWu2IhK1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's construct a simple model with the base BERT encoder together with a few dense layers. Also, I will be using a dropout layer after the BERT encoder layer to reduce overfitting the training data. Although this dropout layer make the model less interpretable, for this practice, I'll still use it here."
      ],
      "metadata": {
        "id": "d78d7eZvJIDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the BERT encoder\n",
        "bert_encoder = keras_nlp.models.BertBackbone.from_preset(\"bert_base_en_uncased\")\n",
        "bert_encoder.trainable = False  # freeze all the parameters in BERT\n",
        "\n",
        "# inputs\n",
        "token_ids = tf.keras.layers.Input(shape=(55,), dtype=tf.int32, name=\"token_ids\")\n",
        "padding_mask = tf.keras.layers.Input(shape=(55,), dtype=tf.int32, name=\"padding_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(55,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "# pass the preprocessed inputs into the BERT encoder\n",
        "encoder_input = {\"token_ids\": token_ids, \"padding_mask\": padding_mask, \"segment_ids\": segment_ids}\n",
        "encoder_output = bert_encoder(encoder_input)\n",
        "\n",
        "# customized dense layers\n",
        "x = tf.keras.layers.Dropout(0.2)(encoder_output[\"pooled_output\"]) # regularize\n",
        "\n",
        "# adding dense layers to reduce the x's dimensions by stages\n",
        "x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dense(48, activation=\"relu\")(x)\n",
        "\n",
        "# output layer\n",
        "output = tf.keras.layers.Dense(5, activation=\"softmax\", name = \"output_layer\")(x)\n",
        "\n",
        "model_BERT_base_custom = tf.keras.Model(inputs=[token_ids, padding_mask, segment_ids],\n",
        "                                 outputs=output,\n",
        "                                 name = \"model_BERT_base_custom\")\n",
        "\n",
        "model_BERT_base_custom.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model_BERT_base_custom.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "8ySR3d4U1U_2",
        "outputId": "26e58116-6057-4eec-f8e1-e534c358f79d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"model_BERT_base_custom\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_BERT_base_custom\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ segment_ids (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bert_backbone             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │    \u001b[38;5;34m109,482,240\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mBertBackbone\u001b[0m)            │ \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m768\u001b[0m)]              │                │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │                        │                │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ bert_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m196,864\u001b[0m │ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │         \u001b[38;5;34m12,336\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ output_layer (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m245\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ segment_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bert_backbone             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">109,482,240</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)]              │                │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │                        │                │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bert_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12,336</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">245</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,691,685\u001b[0m (418.44 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,691,685</span> (418.44 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m209,445\u001b[0m (818.14 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">209,445</span> (818.14 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m109,482,240\u001b[0m (417.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,482,240</span> (417.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_bert = model_BERT_base_custom.fit(train_data_BERT,\n",
        "                                          epochs = 5,\n",
        "                                          validation_data = dev_data_BERT,\n",
        "                                          validation_steps = int(0.1*len(dev_data_BERT)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI0iem891Mo8",
        "outputId": "a4fd00f8-fab3-4403-ca6c-8b7033864cc4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m5627/5627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m723s\u001b[0m 120ms/step - accuracy: 0.3376 - loss: 1.4750 - val_accuracy: 0.3551 - val_loss: 1.4568\n",
            "Epoch 2/5\n",
            "\u001b[1m5627/5627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 114ms/step - accuracy: 0.3520 - loss: 1.4597 - val_accuracy: 0.3521 - val_loss: 1.4397\n",
            "Epoch 3/5\n",
            "\u001b[1m5627/5627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 114ms/step - accuracy: 0.3538 - loss: 1.4579 - val_accuracy: 0.3644 - val_loss: 1.4525\n",
            "Epoch 4/5\n",
            "\u001b[1m5627/5627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 115ms/step - accuracy: 0.3538 - loss: 1.4567 - val_accuracy: 0.3614 - val_loss: 1.4407\n",
            "Epoch 5/5\n",
            "\u001b[1m5627/5627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 115ms/step - accuracy: 0.3541 - loss: 1.4565 - val_accuracy: 0.3537 - val_loss: 1.4515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 customized GPT-2 model"
      ],
      "metadata": {
        "id": "v1Cjn3yCwvS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**!IMPORTANT NOTE!**\n",
        "\n",
        "> PS: similar phenomenon to my experiments on BERT is observed here for my customized GPT-2 model, which also fails to reach beyond $40\\%$ accuracy on the training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "RLDWa36OpaBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor_GPT2 = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\", sequence_length=55)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbEO-rNDZYRi",
        "outputId": "5eeebf22-08c0-40a9-9e67-3772abd2bdca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/config.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 484/484 [00:00<00:00, 1.00MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/tokenizer.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 448/448 [00:00<00:00, 300kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/assets/tokenizer/vocabulary.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 0.99M/0.99M [00:01<00:00, 780kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/assets/tokenizer/merges.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 446k/446k [00:01<00:00, 436kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels))\n",
        "dev_data = tf.data.Dataset.from_tensor_slices((dev_sentences, dev_labels))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels))\n",
        "\n",
        "def preprocess_fn_GPT2(sentences, label):\n",
        "    tokenized = preprocessor_GPT2(sentences)\n",
        "    padding_mask = tf.cast(tokenized != 0, dtype=tf.int32)\n",
        "    return {\"token_ids\": tokenized, \"padding_mask\": padding_mask}, label\n",
        "\n",
        "train_data_GPT2 = train_data.map(preprocess_fn_GPT2).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "dev_data_GPT2 = dev_data.map(preprocess_fn_GPT2).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_data_GPT2 = test_data.map(preprocess_fn_GPT2).batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "v_jT6k_DTsax"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_backbone = keras_nlp.models.GPT2Backbone.from_preset(\"gpt2_base_en\", trainable = False)\n",
        "#gpt2_backbone.trainable = False\n",
        "\n",
        "token_ids = tf.keras.layers.Input(shape=(55,), dtype=tf.int32, name=\"token_ids\")\n",
        "padding_mask = tf.keras.layers.Input(shape=(55,), dtype=tf.int32, name=\"padding_mask\")\n",
        "\n",
        "# Pass inputs through GPT-2 backbone\n",
        "encoder_inputs = {\"token_ids\": token_ids, \"padding_mask\": padding_mask}\n",
        "outputs = gpt2_backbone(encoder_inputs)\n",
        "\n",
        "# Use the last token's embedding for classification\n",
        "last_token_embedding = outputs[:, -1, :]\n",
        "\n",
        "# perform classification\n",
        "x = tf.keras.layers.Dropout(0.1)(last_token_embedding)\n",
        "x = tf.keras.layers.Dense(16, activation=\"relu\")(x)\n",
        "output = tf.keras.layers.Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "model_GPT2 = tf.keras.Model(inputs=[token_ids, padding_mask], outputs=output, name = \"model_GPT2\")\n",
        "\n",
        "model_GPT2.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate = 0.1),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model_GPT2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "NibHUICeMG0g",
        "outputId": "db92b439-de60-4c3c-f485-1bcf77dbf3e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_base_en/2/download/model.weights.h5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 475M/475M [00:30<00:00, 16.1MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"model_GPT2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_GPT2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gpt2_backbone             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m768\u001b[0m)        │    \u001b[38;5;34m124,439,808\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mGPT2Backbone\u001b[0m)            │                        │                │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_1 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ gpt2_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ get_item_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m12,304\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │             \u001b[38;5;34m85\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gpt2_backbone             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)            │                        │                │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gpt2_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ get_item_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,452,197\u001b[0m (474.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,452,197</span> (474.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,389\u001b[0m (48.39 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,389</span> (48.39 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_GPT2 = model_GPT2.fit(train_data_GPT2,\n",
        "                              epochs = 5,\n",
        "                              validation_data = dev_data_GPT2,\n",
        "                              validation_steps = int(0.1*len(dev_data_GPT2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic3RL6sxUGMF",
        "outputId": "6efb82a6-ffd1-45e4-d358-08e85270f823"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m5627/5627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 116ms/step - accuracy: 0.3266 - loss: 1.4780 - val_accuracy: 0.3205 - val_loss: 1.4745\n",
            "Epoch 2/5\n",
            "\u001b[1m5627/5627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 116ms/step - accuracy: 0.3252 - loss: 1.4785 - val_accuracy: 0.3288 - val_loss: 1.4586\n",
            "Epoch 3/5\n",
            "\u001b[1m5627/5627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 116ms/step - accuracy: 0.3252 - loss: 1.4784 - val_accuracy: 0.3195 - val_loss: 1.4725\n",
            "Epoch 4/5\n",
            "\u001b[1m5627/5627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m649s\u001b[0m 115ms/step - accuracy: 0.3253 - loss: 1.4785 - val_accuracy: 0.3394 - val_loss: 1.4613\n",
            "Epoch 5/5\n",
            "\u001b[1m5627/5627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m649s\u001b[0m 115ms/step - accuracy: 0.3253 - loss: 1.4780 - val_accuracy: 0.3291 - val_loss: 1.4694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Conjectures and reasons for failing to achieve an accuracy beyond $40$ percent\n",
        "\n",
        "\n",
        "1. The pretrained transformer encoder layers yield a dimension much larger than the dimension of relevant information, which is $55$ for our data. The large dimensions results in a very slow convergence rate when fitting the model with our training data.\n",
        "\n",
        "2. Moreover, I am only running the BERT layer for a few epochs ($\\sim 10$). In fact, it has been shown that to achieve a good accuracy with, for example, BERT encoder, one may need to train for hundreds of epochs [[reference](https://www.kaggle.com/models/google/experts-bert/tensorFlow2/pubmed/2?tfhub-redirect=true)]. However, due to limited computation budget on Google Colab, I decided not to continue experimenting on these models.\n",
        "\n"
      ],
      "metadata": {
        "id": "UX6fIN5uwzAz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TA8K5RzkuFG1"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}